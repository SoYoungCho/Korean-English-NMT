{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "import pandas as pd\n",
    "import MeCab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 조사 제거해서 문장 반환하는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Korean_tokenizer(x): \n",
    "    \n",
    "    m = MeCab.Tagger()\n",
    "    delete_tag = ['BOS/EOS', 'JKS', 'JKC', 'JKG', 'JKO', 'JKB', 'JKV', 'JKQ', 'JX', 'JC']\n",
    "\n",
    "    \n",
    "    \n",
    "    def remove_josa(sentence):\n",
    "        sentence_split = sentence.split() # 원본 문장 띄어쓰기로 분리\n",
    "\n",
    "        dict_list = []\n",
    "\n",
    "        for token in sentence_split: # 띄어쓰기로 분리된 각 토큰 {'단어':'형태소 태그'} 와 같이 딕셔너리 생성\n",
    "            m.parse('')\n",
    "            node = m.parseToNode(token)\n",
    "            word_list = []\n",
    "            pos_list = []\n",
    "        \n",
    "            while node:\n",
    "                morphs = node.feature.split(',')\n",
    "                word_list.append(node.surface)\n",
    "                pos_list.append(morphs[0])\n",
    "                node = node.next\n",
    "            dict_list.append(dict(zip(word_list, pos_list)))        \n",
    "\n",
    "        for dic in dict_list: # delete_tag에 해당하는 단어 쌍 지우기 (조사에 해당하는 단어 지우기)\n",
    "            for key in list(dic.keys()):\n",
    "                if dic[key] in delete_tag:\n",
    "                    del dic[key]\n",
    "\n",
    "        combine_word = [''.join(list(dic.keys())) for dic in dict_list] # 형태소로 분리된 각 단어 합치기\n",
    "        result = ' '.join(combine_word) # 띄어쓰기로 분리된 각 토큰 합치기\n",
    "\n",
    "        return result # 온전한 문장을 반환\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    df = pd.read_csv(\"C:/Users/Soyoung Cho/Desktop/NMT Project/dataset/datalist_striped.csv\")\n",
    "    KOR_data = df['Korean']\n",
    "    \n",
    "    f = open(\"kor_no_josa.txt\", \"w\", encoding = 'utf-8')\n",
    "    for row in KOR_data[:100000]:\n",
    "        f.write(remove_josa(row)) # 조사 제거한 문장 저장\n",
    "        f.write('\\n')\n",
    "    f.close()\n",
    "    \n",
    "    spm.SentencePieceTrainer.Train('--input=kor_no_josa.txt \\\n",
    "                               --model_prefix=revise \\\n",
    "                               --vocab_size=100000 \\\n",
    "                               --hard_vocab_limit=false')\n",
    "    \n",
    "    sp = spm.SentencePieceProcessor()\n",
    "    sp.Load('revise.model')\n",
    "    print(\"success\")\n",
    "    sentence_piece = sp.EncodeAsPieces(x)\n",
    "    \n",
    "    return sentence_piece\n",
    " \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def English_tokenizer():\n",
    "    data = pd.read_csv(\"C:/Users/Soyoung Cho/Desktop/NMT Project/dataset/datalist_striped.csv\")\n",
    "    ENG_data = data['English']\n",
    "\n",
    "    f = open(\"eng.txt\", \"w\", encoding = 'utf-8')\n",
    "    for row in ENG_data[:100000]:\n",
    "        f.write(row)\n",
    "        f.write('\\n')\n",
    "        f.close()\n",
    "    spm.SentencePieceTrainer.Train('--input=eng.txt \\\n",
    "                           --model_prefix=revise \\\n",
    "                           --vocab_size=100000 \\\n",
    "                           --hard_vocab_limit=false')\n",
    "    sp = spm.SentencePieceProcessor()\n",
    "    sp.Load('revise.model')\n",
    "    return lambda x: sp.EncodeAsPieces(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"너 아침밥 먹음?\"\n",
    "m = MeCab.Tagger()\n",
    "delete_tag = ['BOS/EOS', 'JKS', 'JKC', 'JKG', 'JKO', 'JKB', 'JKV', 'JKQ', 'JX', 'JC']\n",
    "\n",
    "sentence_split = sentence.split() # 원본 문장 띄어쓰기로 분리\n",
    "\n",
    "dict_list = []\n",
    "\n",
    "for token in sentence_split: # 띄어쓰기로 분리된 각 토큰 {'단어':'형태소 태그'} 와 같이 딕셔너리 생성\n",
    "    m.parse('')\n",
    "    node = m.parseToNode(token)\n",
    "    word_list = []\n",
    "    pos_list = []\n",
    "    while node:\n",
    "        morphs = node.feature.split(',')\n",
    "        word_list.append(node.surface)\n",
    "        pos_list.append(morphs[0])\n",
    "        node = node.next\n",
    "    dict_list.append(dict(zip(word_list, pos_list)))   \n",
    "    \n",
    "for dic in dict_list: # delete_tag에 해당하는 단어 쌍 지우기 (조사에 해당하는 단어 지우기)\n",
    "    for key in list(dic.keys()):\n",
    "        if dic[key] in delete_tag:\n",
    "            del dic[key]\n",
    "\n",
    "combine_word = [''.join(list(dic.keys())) for dic in dict_list] # 형태소로 분리된 각 단어 합치기\n",
    "result = ' '.join(combine_word) # 띄어쓰기로 분리된 각 토큰 합치기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'너': 'NP'}, {'아침밥': 'NNG'}, {'먹': 'VV', '음': 'ETN', '?': 'SF'}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'안녕하세요. 감사합니다'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = MeCab.Tagger()\n",
    "delete_tag = ['BOS/EOS', 'JKS', 'JKC', 'JKG', 'JKO', 'JKB', 'JKV', 'JKQ', 'JX', 'JC']\n",
    "'''\n",
    "\"JKS\":     \"주격 조사\",\n",
    "\"JKC\":     \"보격 조사\",\n",
    "\"JKG\":     \"관형격 조사\",\n",
    "\"JKO\":     \"목적격 조사\",`\n",
    "\"JKB\":     \"부사격 조사\",\n",
    "\"JKV\":     \"호격 조사\",\n",
    "\"JKQ\":     \"인용격 조사\",\n",
    "\"JX\":      \"보조사\",\n",
    "\"JC\":      \"접속 조사\",\n",
    "'''\n",
    "\n",
    "def remove_josa(sentence):\n",
    "    sentence_split = sentence.split() # 원본 문장 띄어쓰기로 분리\n",
    "    \n",
    "    dict_list = []\n",
    "    \n",
    "    for token in sentence_split: # 띄어쓰기로 분리된 각 토큰 {'단어':'형태소 태그'} 와 같이 딕셔너리 생성\n",
    "        m.parse('')\n",
    "        node = m.parseToNode(token)\n",
    "        word_list = []\n",
    "        pos_list = []\n",
    "        while node:\n",
    "            morphs = node.feature.split(',')\n",
    "            word_list.append(node.surface)\n",
    "            pos_list.append(morphs[0])\n",
    "            node = node.next\n",
    "        dict_list.append(dict(zip(word_list, pos_list)))        \n",
    "\n",
    "    for dic in dict_list: # delete_tag에 해당하는 단어 쌍 지우기 (조사에 해당하는 단어 지우기)\n",
    "        for key in list(dic.keys()):\n",
    "            if dic[key] in delete_tag:\n",
    "                del dic[key]\n",
    "    \n",
    "    combine_word = [''.join(list(dic.keys())) for dic in dict_list] # 형태소로 분리된 각 단어 합치기\n",
    "    result = ' '.join(combine_word) # 띄어쓰기로 분리된 각 토큰 합치기\n",
    "\n",
    "    return result # 온전한 문장을 반환\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "예시 ) 너는  \n",
    "\n",
    "--node--  \n",
    "morphs: ['BOS/EOS', '*', '*', '*', '*', '*', '*', '*']  \n",
    "word_list :  ['']  \n",
    "pos_list :  ['BOS/EOS']  \n",
    "\n",
    "--node--  \n",
    "morphs: ['NP', '*', 'F', '너', '*', '*', '*', '*']  \n",
    "word_list :  ['', '너']  \n",
    "pos_list :  ['BOS/EOS', 'NP']  \n",
    "\n",
    "--node--  \n",
    "morphs: ['JX', '*', 'T', '는', '*', '*', '*', '*']  \n",
    "word_list :  ['', '너', '는']  \n",
    "pos_list :  ['BOS/EOS', 'NP', 'JX']  \n",
    "\n",
    "--node--  \n",
    "morphs: ['BOS/EOS', '*', '*', '*', '*', '*', '*', '*']  \n",
    "word_list :  ['', '너', '는', '']    \n",
    "pos_list :  ['BOS/EOS', 'NP', 'JX', 'BOS/EOS']  \n",
    "dict_list,  [{'': 'BOS/EOS', '너': 'NP', '는': 'JX'}]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sentencepiece 적용 (original과 조사 제거 버전 비교)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "KOR_data = pd.read_csv(\"C:/Users/Soyoung Cho/Desktop/NMT Project/dataset/datalist_striped.csv\", encoding = 'utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "KOR_data = KOR_data['Korean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = open(\"original.txt\", \"w\", encoding = 'utf-8')\n",
    "f2 = open(\"no_josa.txt\", \"w\", encoding = 'utf-8')\n",
    "\n",
    "for row in KOR_data[:100000]:\n",
    "    f1.write(row) # 조사 제거 안한 원본 문장 저장\n",
    "    f1.write('\\n')\n",
    "    \n",
    "    f2.write(remove_josa(row)) # 조사 제거한 문장 저장\n",
    "    f2.write('\\n')\n",
    "#f.close()\n",
    "#f2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# original 모델 생성 (조사 제거안한 문장으로 학습)\n",
    "spm.SentencePieceTrainer.Train('--input=original.txt \\\n",
    "                               --model_prefix=original \\\n",
    "                               --vocab_size=100000 \\\n",
    "                               --hard_vocab_limit=false')\n",
    "\n",
    "# 조사 제거한 문장으로 모델 생성\n",
    "spm.SentencePieceTrainer.Train('--input=no_josa.txt \\\n",
    "                               --model_prefix=revise \\\n",
    "                               --vocab_size=100000 \\\n",
    "                               --hard_vocab_limit=false')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#모델 불러오기\n",
    "sp1 = spm.SentencePieceProcessor()\n",
    "sp1.Load('original.model')\n",
    "\n",
    "sp2 = spm.SentencePieceProcessor()\n",
    "sp2.Load('revise.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<원본 문장>\n",
      "안녕하세요. 감사합니다\n",
      "\n",
      "<original 모델>\n",
      "['▁안녕하세요', '.', '▁감사합니다']\n",
      "\n",
      "<조사 제거 모델>\n",
      "['▁안녕하세요', '.', '▁감사합니다']\n"
     ]
    }
   ],
   "source": [
    "# input setence \n",
    "#sentence = \"시 관계자는 선별진료소, 의료기관 등에 개인보호구인 덧신, 장갑·고글, 마스크 800개씩을 배포했지만 물량이 턱없이 부족하다며 정부에 추가 지원을 요청했다.\"\n",
    "sentence = \"안녕하세요. 감사합니다\"\n",
    "\n",
    "# 0. 원본 문장\n",
    "print(\"<원본 문장>\")\n",
    "print(sentence+'\\n')\n",
    "\n",
    "# 1. original 모델 (조사 제거 안한 문장으로 학습)\n",
    "print(\"<original 모델>\")\n",
    "print(sp1.EncodeAsPieces(sentence))\n",
    "\n",
    "# 1. 조사 제거 모델  (조사 제거 한 문장으로 학습)\n",
    "print(\"\\n<조사 제거 모델>\")\n",
    "print(sp2.EncodeAsPieces(sentence))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
