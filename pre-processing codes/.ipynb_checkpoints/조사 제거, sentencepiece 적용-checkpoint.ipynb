{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "import pandas as pd\n",
    "import MeCab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 조사 제거해서 문장 반환하는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = MeCab.Tagger()\n",
    "delete_tag = ['BOS/EOS', 'JKS', 'JKC', 'JKG', 'JKO', 'JKB', 'JKV', 'JKQ', 'JX', 'JC']\n",
    "'''\n",
    "\"JKS\":     \"주격 조사\",\n",
    "\"JKC\":     \"보격 조사\",\n",
    "\"JKG\":     \"관형격 조사\",\n",
    "\"JKO\":     \"목적격 조사\",\n",
    "\"JKB\":     \"부사격 조사\",\n",
    "\"JKV\":     \"호격 조사\",\n",
    "\"JKQ\":     \"인용격 조사\",\n",
    "\"JX\":      \"보조사\",\n",
    "\"JC\":      \"접속 조사\",\n",
    "'''\n",
    "\n",
    "def remove_josa(sentence):\n",
    "    sentence_split = sentence.split() # 원본 문장 띄어쓰기로 분리\n",
    "    \n",
    "    dict_list = []\n",
    "    \n",
    "    for token in sentence_split: # 띄어쓰기로 분리된 각 토큰 {'단어':'형태소 태그'} 와 같이 딕셔너리 생성\n",
    "        m.parse('')\n",
    "        node = m.parseToNode(token)\n",
    "        word_list = []\n",
    "        pos_list = []\n",
    "        while node:\n",
    "            morphs = node.feature.split(',')\n",
    "            word_list.append(node.surface)\n",
    "            pos_list.append(morphs[0])\n",
    "            node = node.next\n",
    "        dict_list.append(dict(zip(word_list, pos_list)))        \n",
    "\n",
    "    for dic in dict_list: # delete_tag에 해당하는 단어 쌍 지우기 (조사에 해당하는 단어 지우기)\n",
    "        for key in list(dic.keys()):\n",
    "            if dic[key] in delete_tag:\n",
    "                del dic[key]\n",
    "    \n",
    "    combine_word = [''.join(list(dic.keys())) for dic in dict_list] # 형태소로 분리된 각 단어 합치기\n",
    "    result = ' '.join(combine_word) # 띄어쓰기로 분리된 각 토큰 합치기\n",
    "\n",
    "    return result # 온전한 문장을 반환\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "예시 ) 너는  \n",
    "\n",
    "--node--  \n",
    "morphs: ['BOS/EOS', '*', '*', '*', '*', '*', '*', '*']  \n",
    "word_list :  ['']  \n",
    "pos_list :  ['BOS/EOS']  \n",
    "\n",
    "--node--  \n",
    "morphs: ['NP', '*', 'F', '너', '*', '*', '*', '*']  \n",
    "word_list :  ['', '너']  \n",
    "pos_list :  ['BOS/EOS', 'NP']  \n",
    "\n",
    "--node--  \n",
    "morphs: ['JX', '*', 'T', '는', '*', '*', '*', '*']  \n",
    "word_list :  ['', '너', '는']  \n",
    "pos_list :  ['BOS/EOS', 'NP', 'JX']  \n",
    "\n",
    "--node--  \n",
    "morphs: ['BOS/EOS', '*', '*', '*', '*', '*', '*', '*']  \n",
    "word_list :  ['', '너', '는', '']    \n",
    "pos_list :  ['BOS/EOS', 'NP', 'JX', 'BOS/EOS']  \n",
    "dict_list,  [{'': 'BOS/EOS', '너': 'NP', '는': 'JX'}]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sentencepiece 적용 (original과 조사 제거 버전 비교)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "KOR_data = pd.read_csv(\"C:/Users/Soyoung Cho/Desktop/NMT Project/dataset/datalist_striped.csv\", encoding = 'utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "KOR_data = KOR_data['Korean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = open(\"original.txt\", \"w\", encoding = 'utf-8')\n",
    "f2 = open(\"no_josa.txt\", \"w\", encoding = 'utf-8')\n",
    "\n",
    "for row in KOR_data[:100000]:\n",
    "    f1.write(row) # 조사 제거 안한 원본 문장 저장\n",
    "    f1.write('\\n')\n",
    "    \n",
    "    f2.write(remove_josa(row)) # 조사 제거한 문장 저장\n",
    "    f2.write('\\n')\n",
    "#f.close()\n",
    "#f2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# original 모델 생성 (조사 제거안한 문장으로 학습)\n",
    "spm.SentencePieceTrainer.Train('--input=original.txt \\\n",
    "                               --model_prefix=original \\\n",
    "                               --vocab_size=100000 \\\n",
    "                               --hard_vocab_limit=false')\n",
    "\n",
    "# 조사 제거한 문장으로 모델 생성\n",
    "spm.SentencePieceTrainer.Train('--input=no_josa.txt \\\n",
    "                               --model_prefix=revise \\\n",
    "                               --vocab_size=100000 \\\n",
    "                               --hard_vocab_limit=false')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#모델 불러오기\n",
    "sp1 = spm.SentencePieceProcessor()\n",
    "sp1.Load('original.model')\n",
    "\n",
    "sp2 = spm.SentencePieceProcessor()\n",
    "sp2.Load('revise.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<원본 문장>\n",
      "시 관계자는 선별진료소, 의료기관 등에 개인보호구인 덧신, 장갑·고글, 마스크 800개씩을 배포했지만 물량이 턱없이 부족하다며 정부에 추가 지원을 요청했다.\n",
      "\n",
      "<original 모델>\n",
      "['▁시', '▁관계자는', '▁선별', '진료소', ',', '▁의료기관', '▁등에', '▁개인', '보호', '구인', '▁덧', '신', ',', '▁장갑', '·', '고', '글', ',', '▁마스크', '▁800', '개씩', '을', '▁배포', '했지만', '▁물량이', '▁턱없', '이', '▁부족하다', '며', '▁정부에', '▁추가', '▁지원', '을', '▁요청했다', '.']\n",
      "\n",
      "<조사 제거 모델>\n",
      "['▁시', '▁관계자', '는', '▁', '선별진료', '소', ',', '▁의료기관', '▁등', '에', '▁개인', '보호', '구인', '▁덧', '신', ',', '▁장갑', '·', '고', '글', ',', '▁마스크', '▁800', '개씩', '을', '▁배포', '했지만', '▁물량', '이', '▁턱없', '이', '▁부족하다', '며', '▁정부', '에', '▁추가', '▁지원', '을', '▁요청했다', '.']\n"
     ]
    }
   ],
   "source": [
    "# input setence \n",
    "sentence = \"시 관계자는 선별진료소, 의료기관 등에 개인보호구인 덧신, 장갑·고글, 마스크 800개씩을 배포했지만 물량이 턱없이 부족하다며 정부에 추가 지원을 요청했다.\"\n",
    "\n",
    "# 0. 원본 문장\n",
    "print(\"<원본 문장>\")\n",
    "print(sentence+'\\n')\n",
    "\n",
    "# 1. original 모델 (조사 제거 안한 문장으로 학습)\n",
    "print(\"<original 모델>\")\n",
    "print(sp1.EncodeAsPieces(sentence))\n",
    "\n",
    "# 1. 조사 제거 모델  (조사 제거 한 문장으로 학습)\n",
    "print(\"\\n<조사 제거 모델>\")\n",
    "print(sp2.EncodeAsPieces(sentence))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
