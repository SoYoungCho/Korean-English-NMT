{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seq2seq'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-dbc2ae427bf0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorchtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mseq2seq\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mseq2seq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainer\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSupervisedTrainer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mseq2seq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mEncoderRNN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDecoderRNN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSeq2seq\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'seq2seq'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import torchtext\n",
    "\n",
    "import seq2seq\n",
    "from seq2seq.trainer import SupervisedTrainer\n",
    "from seq2seq.models import EncoderRNN, DecoderRNN, Seq2seq\n",
    "from seq2seq.loss import Perplexity\n",
    "from seq2seq.optim import Optimizer\n",
    "from seq2seq.dataset import SourceField, TargetField\n",
    "from seq2seq.evaluator import Predictor\n",
    "from seq2seq.util.checkpoint import Checkpoint\n",
    "\n",
    "try:\n",
    "    raw_input          # Python 2\n",
    "except NameError:\n",
    "    raw_input = input  # Python 3\n",
    "\n",
    "# Sample usage:\n",
    "#     # training\n",
    "#     python examples/sample.py --train_path $TRAIN_PATH --dev_path $DEV_PATH --expt_dir $EXPT_PATH\n",
    "#     # resuming from the latest checkpoint of the experiment\n",
    "#      python examples/sample.py --train_path $TRAIN_PATH --dev_path $DEV_PATH --expt_dir $EXPT_PATH --resume\n",
    "#      # resuming from a specific checkpoint\n",
    "#      python examples/sample.py --train_path $TRAIN_PATH --dev_path $DEV_PATH --expt_dir $EXPT_PATH --load_checkpoint $CHECKPOINT_DIR\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--train_path', action='store', dest='train_path',\n",
    "                    help='Path to train data')\n",
    "parser.add_argument('--dev_path', action='store', dest='dev_path',\n",
    "                    help='Path to dev data')\n",
    "parser.add_argument('--expt_dir', action='store', dest='expt_dir', default='./experiment',\n",
    "                    help='Path to experiment directory. If load_checkpoint is True, then path to checkpoint directory has to be provided')\n",
    "parser.add_argument('--load_checkpoint', action='store', dest='load_checkpoint',\n",
    "                    help='The name of the checkpoint to load, usually an encoded time string')\n",
    "parser.add_argument('--resume', action='store_true', dest='resume',\n",
    "                    default=False,\n",
    "                    help='Indicates if training has to be resumed from the latest checkpoint')\n",
    "parser.add_argument('--log-level', dest='log_level',\n",
    "                    default='info',\n",
    "                    help='Logging level.')\n",
    "\n",
    "opt = parser.parse_args()\n",
    "\n",
    "LOG_FORMAT = '%(asctime)s %(name)-12s %(levelname)-8s %(message)s'\n",
    "logging.basicConfig(format=LOG_FORMAT, level=getattr(logging, opt.log_level.upper()))\n",
    "logging.info(opt)\n",
    "\n",
    "if opt.load_checkpoint is not None:\n",
    "    logging.info(\"loading checkpoint from {}\".format(os.path.join(opt.expt_dir, Checkpoint.CHECKPOINT_DIR_NAME, opt.load_checkpoint)))\n",
    "    checkpoint_path = os.path.join(opt.expt_dir, Checkpoint.CHECKPOINT_DIR_NAME, opt.load_checkpoint)\n",
    "    checkpoint = Checkpoint.load(checkpoint_path)\n",
    "    seq2seq = checkpoint.model\n",
    "    input_vocab = checkpoint.input_vocab\n",
    "    output_vocab = checkpoint.output_vocab\n",
    "else:\n",
    "    # Prepare dataset\n",
    "    src = SourceField()\n",
    "    tgt = TargetField()\n",
    "    max_len = 50\n",
    "    def len_filter(example):\n",
    "        return len(example.src) <= max_len and len(example.tgt) <= max_len\n",
    "    train = torchtext.data.TabularDataset(\n",
    "        path=opt.train_path, format='tsv',\n",
    "        fields=[('src', src), ('tgt', tgt)],\n",
    "        filter_pred=len_filter\n",
    "    )\n",
    "    dev = torchtext.data.TabularDataset(\n",
    "        path=opt.dev_path, format='tsv',\n",
    "        fields=[('src', src), ('tgt', tgt)],\n",
    "        filter_pred=len_filter\n",
    "    )\n",
    "    src.build_vocab(train, max_size=50000)\n",
    "    tgt.build_vocab(train, max_size=50000)\n",
    "    input_vocab = src.vocab\n",
    "    output_vocab = tgt.vocab\n",
    "\n",
    "    # NOTE: If the source field name and the target field name\n",
    "    # are different from 'src' and 'tgt' respectively, they have\n",
    "    # to be set explicitly before any training or inference\n",
    "    # seq2seq.src_field_name = 'src'\n",
    "    # seq2seq.tgt_field_name = 'tgt'\n",
    "\n",
    "    # Prepare loss\n",
    "    weight = torch.ones(len(tgt.vocab))\n",
    "    pad = tgt.vocab.stoi[tgt.pad_token]\n",
    "    loss = Perplexity(weight, pad)\n",
    "    if torch.cuda.is_available():\n",
    "        loss.cuda()\n",
    "\n",
    "    seq2seq = None\n",
    "    optimizer = None\n",
    "    if not opt.resume:\n",
    "        # Initialize model\n",
    "        hidden_size=128\n",
    "        bidirectional = True\n",
    "        encoder = EncoderRNN(len(src.vocab), max_len, hidden_size,\n",
    "                             bidirectional=bidirectional, variable_lengths=True)\n",
    "        decoder = DecoderRNN(len(tgt.vocab), max_len, hidden_size * 2 if bidirectional else hidden_size,\n",
    "                             dropout_p=0.2, use_attention=True, bidirectional=bidirectional,\n",
    "                             eos_id=tgt.eos_id, sos_id=tgt.sos_id)\n",
    "        seq2seq = Seq2seq(encoder, decoder)\n",
    "        if torch.cuda.is_available():\n",
    "            seq2seq.cuda()\n",
    "\n",
    "        for param in seq2seq.parameters():\n",
    "            param.data.uniform_(-0.08, 0.08)\n",
    "\n",
    "        # Optimizer and learning rate scheduler can be customized by\n",
    "        # explicitly constructing the objects and pass to the trainer.\n",
    "        #\n",
    "        # optimizer = Optimizer(torch.optim.Adam(seq2seq.parameters()), max_grad_norm=5)\n",
    "        # scheduler = StepLR(optimizer.optimizer, 1)\n",
    "        # optimizer.set_scheduler(scheduler)\n",
    "\n",
    "    # train\n",
    "    t = SupervisedTrainer(loss=loss, batch_size=32,\n",
    "                          checkpoint_every=50,\n",
    "                          print_every=10, expt_dir=opt.expt_dir)\n",
    "\n",
    "    seq2seq = t.train(seq2seq, train,\n",
    "                      num_epochs=6, dev_data=dev,\n",
    "                      optimizer=optimizer,\n",
    "                      teacher_forcing_ratio=0.5,\n",
    "                      resume=opt.resume)\n",
    "\n",
    "predictor = Predictor(seq2seq, input_vocab, output_vocab)\n",
    "\n",
    "while True:\n",
    "    seq_str = raw_input(\"Type in a source sequence:\")\n",
    "    seq = seq_str.strip().split()\n",
    "    print(predictor.predict(seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
