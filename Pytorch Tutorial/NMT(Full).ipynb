{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data_loader.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torchtext import data, datasets\n",
    "\n",
    "PAD = 1\n",
    "BOS = 2\n",
    "EOS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "\n",
    "    def __init__(self, train_fn = None, \n",
    "                    valid_fn = None, \n",
    "                    exts = None,\n",
    "                    batch_size = 64, \n",
    "                    device = 'cpu', \n",
    "                    max_vocab = 99999999,    \n",
    "                    max_length = 255, \n",
    "                    fix_length = None, \n",
    "                    use_bos = True, \n",
    "                    use_eos = True, \n",
    "                    shuffle = True\n",
    "                    ):\n",
    "\n",
    "        super(DataLoader, self).__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        * sequential : 데이터의 유형이 연속형 데이터인지(False면 토큰화가 적용x)\n",
    "        * use_vocab : Vocab 사용 여부(False면 필드의 데이터는 이미 숫자여야함)\n",
    "        * batch_first : 배치 수가 먼저 텐서를 생성할지 여부\n",
    "        * include_lengths : 패딩 된 미니 배치의 튜플과 각 예제의 길이를 포함하는 목록 \n",
    "                            또는 패딩 된 미니 배치를 반환할지 여부(default: False)\n",
    "        * fix_length : 모든 문장이 채워지는 고정 길이, 유연한 sequence의 경우 None\n",
    "        * init_token : 모든 문장 앞에 추가되는 토큰\n",
    "        * eos_token : 모든 문장 뒤에 추가되는 토큰\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        self.src = data.Field(sequential = True,\n",
    "                                use_vocab = True, \n",
    "                                batch_first = True, \n",
    "                                include_lengths = True, \n",
    "                                fix_length = fix_length, \n",
    "                                init_token = None, \n",
    "                                eos_token = None\n",
    "                                )\n",
    "        super(DataLoader, self).__init__()\n",
    "\n",
    "        self.tgt = data.Field(sequential = True, \n",
    "                                use_vocab = True, \n",
    "                                batch_first = True, \n",
    "                                include_lengths = True, \n",
    "                                fix_length = fix_length, \n",
    "                                init_token = '<BOS>' if use_bos else None, \n",
    "                                eos_token = '<EOS>' if use_eos else None\n",
    "                                )\n",
    "        \n",
    "        \n",
    "        \n",
    "#         if train_fn is not None and valid_fn is not None and exts is not None:\n",
    "            \n",
    "        train = TranslationDataset(path = train_fn, exts = exts,\n",
    "                                        fields = [('src', self.src), ('tgt', self.tgt)], \n",
    "                                        max_length = max_length\n",
    "                                        )\n",
    "        valid = TranslationDataset(path = valid_fn, exts = exts,\n",
    "                                        fields = [('src', self.src), ('tgt', self.tgt)], \n",
    "                                        max_length = max_length\n",
    "                                        )\n",
    "\n",
    "        self.train_iter = data.BucketIterator(train, \n",
    "                                                batch_size = batch_size, \n",
    "                                                shuffle = shuffle, \n",
    "                                                sort_key=lambda x: len(x.tgt) + (max_length * len(x.src)), \n",
    "                                                sort_within_batch = True\n",
    "                                                )\n",
    "\n",
    "        self.valid_iter = data.BucketIterator(valid, \n",
    "                                                batch_size = batch_size, \n",
    "                                                shuffle = False, \n",
    "                                                sort_key=lambda x: len(x.tgt) + (max_length * len(x.src)), \n",
    "                                                sort_within_batch = True\n",
    "                                                )\n",
    "\n",
    "        self.src.build_vocab(train, max_size = max_vocab)\n",
    "        self.tgt.build_vocab(train, max_size = max_vocab)\n",
    "\n",
    "    def load_vocab(self, src_vocab, tgt_vocab):\n",
    "        self.src.vocab = src_vocab\n",
    "        self.tgt.vocab = tgt_vocab\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defines a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'cp949' codec can't decode byte 0xeb in position 25: illegal multibyte sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-33fdd2f4cd85>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     44\u001b[0m     loader = DataLoader('C:/Users/Soyoung Cho/Desktop/NMT Project/dataset/', 'C:/Users/Soyoung Cho/Desktop/NMT Project/dataset/', ('train.csv', 'test.csv'),\n\u001b[0;32m     45\u001b[0m                             \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m                             \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m                             )\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-ff699e14049f>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, train_fn, valid_fn, exts, batch_size, device, max_vocab, max_length, fix_length, use_bos, use_eos, shuffle)\u001b[0m\n\u001b[0;32m     53\u001b[0m         train = TranslationDataset(path = train_fn, exts = exts,\n\u001b[0;32m     54\u001b[0m                                         \u001b[0mfields\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'src'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'tgt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtgt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m                                         \u001b[0mmax_length\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m                                         )\n\u001b[0;32m     57\u001b[0m         valid = TranslationDataset(path = valid_fn, exts = exts,\n",
      "\u001b[1;32m<ipython-input-3-33fdd2f4cd85>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, path, exts, fields, max_length, **kwargs)\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mexamples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc_path\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msrc_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrg_path\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtrg_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0msrc_line\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrg_line\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrg_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m                 \u001b[0msrc_line\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrg_line\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msrc_line\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrg_line\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mmax_length\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mmax_length\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc_line\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrg_line\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'cp949' codec can't decode byte 0xeb in position 25: illegal multibyte sequence"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "class TranslationDataset(data.Dataset):\n",
    "\n",
    "    def sort_key(ex):  # 음수와 양수 모두 가능\n",
    "        return data.interleave_keys(len(ex.src), len(ex.trg))\n",
    "\n",
    "    def __init__(self, path, exts, fields, max_length=None, **kwargs):\n",
    "        \n",
    "        \"\"\"\n",
    "        * path : 두 언어의 데이터 파일 경로\n",
    "        * exts : 각 언어의 경로 확장을 포함하는 튜플\n",
    "        * fields : 각 언어의 데이터에 사용될 필드를 포함하는 튜플\n",
    "        * **kwargs : 생성자에 전달 \n",
    "        \"\"\"\n",
    "        if not isinstance(fields[0], (tuple, list)):\n",
    "            fields = [('src', fields[0]), ('trg', fields[1])]\n",
    "\n",
    "#         if not path.endswith('.'):\n",
    "#             path += '.'\n",
    "\n",
    "        src_path, trg_path = tuple(os.path.expanduser(path + x) for x in exts)\n",
    "        \n",
    "        examples = []\n",
    "        with open(src_path) as src_file, open(trg_path) as trg_file:\n",
    "            for src_line, trg_line in zip(src_file, trg_file):\n",
    "                src_line, trg_line = src_line.strip(), trg_line.strip()\n",
    "                if max_length and max_length < max(len(src_line.split()), len(trg_line.split())):\n",
    "                    continue\n",
    "                if src_line != '' and trg_line != '':\n",
    "                    examples.append(data.Example.fromlist(\n",
    "                        [src_line, trg_line], fields))\n",
    "\n",
    "        super(TranslationDataset, self).__init__(examples, fields, **kwargs)\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "\n",
    "    \"\"\"\n",
    "    argv1,2 : train.csv와 test.csv파일이 있는 공통 경로\n",
    "    (argv3, argv4) : 확장자를 포함한 각 파일 이름\n",
    "    \n",
    "    \"\"\"\n",
    "    loader = DataLoader('C:/Users/Soyoung Cho/Desktop/NMT Project/dataset/', 'C:/Users/Soyoung Cho/Desktop/NMT Project/dataset/', ('train.csv', 'test.csv'),\n",
    "                            shuffle=False,\n",
    "                            batch_size=8\n",
    "                            )\n",
    "    \n",
    "    \n",
    "    print(len(loader.src.vocab))\n",
    "    print(len(loader.tgt.vocab))\n",
    "\n",
    "    for batch_index, batch in enumerate(loader.train_iter):\n",
    "        print(batch.src)\n",
    "        print(batch.tgt)\n",
    "\n",
    "        if batch_index > 1:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from data_loader import DataLoader\n",
    "import data_loader\n",
    "from simple_nmt.seq2seq import Seq2Seq\n",
    "import simple_nmt.trainer as trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://books.google.co.kr/books?id=LV2nDwAAQBAJ&pg=PA145&lpg=PA145&dq=__init__(self,+train_fn+%3D+None&source=bl&ots=uhXWUwlTcz&sig=ACfU3U0AcrutNKVXJ19pieVc0xctnQOzTA&hl=ko&sa=X&ved=2ahUKEwj6u4-fuM7nAhXYP3AKHfKuDNoQ6AEwAHoECAsQAQ#v=onepage&q=__init__(self%2C%20train_fn%20%3D%20None&f=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
